#Assignment-1

#Q1. Write a program to implement breadth first search traversal.

from collections import deque

def bfs(graph, start):
    queue = deque([start])
    visited = set([start])
    bfs_result = []

    while queue:
        vertex = queue.popleft()
        bfs_result.append(vertex)

        for neighbor in graph[vertex]:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append(neighbor)

    return bfs_result

graph = {
    'A': ['B', 'C'],
    'B': ['A', 'D', 'E'],
    'C': ['A', 'F'],
    'D': ['B'],
    'E': ['B', 'F'],
    'F': ['C', 'E']
}

start_node = 'A'

bfs_traversal = bfs(graph, start_node)
print("BFS Traversal starting from node", start_node, ":", bfs_traversal)


#Q.2.Write a program to implement depth first search traversal. 

def dfs(graph, start, visited=None):
    if visited is None:
        visited = set()  # Initialize the visited set on the first call

    visited.add(start)
 
    print(start, end=" ")

    for neighbor in graph[start]:
        if neighbor not in visited:
            dfs(graph, neighbor, visited)

graph = {
    'A': ['B', 'C'],
    'B': ['A', 'D', 'E'],
    'C': ['A', 'F'],
    'D': ['B'],
    'E': ['B', 'F'],
    'F': ['C', 'E']
}

start_node = 'A'

print("DFS Traversal starting from node", start_node, ":")
dfs(graph, start_node)


#Q.3.Write a program to implement A* Algorithm. 

import heapq
class Node:
    def __init__(self, name, parent=None, g=0, h=0):
        self.name = name       # Name of the node
        self.parent = parent   # Parent node
        self.g = g             # Cost to reach the node
        self.h = h             # Heuristic estimate to the goal
        self.f = g + h         # Total cost (g + h)

    def __lt__(self, other):
        return self.f < other.f

# A* Algorithm
def a_star_algorithm(graph, heuristic, start, goal):
    open_list = []
    closed_list = set()

    start_node = Node(start, None, 0, heuristic[start])
    heapq.heappush(open_list, start_node)

    while open_list:
        current_node = heapq.heappop(open_list)
        closed_list.add(current_node.name)

        if current_node.name == goal:
            path = []
            while current_node:
                path.append(current_node.name)
                current_node = current_node.parent
            return path[::-1]  # Return reversed path

        for neighbor, cost in graph[current_node.name]:
            if neighbor in closed_list:
                continue  # Skip if the neighbor has been evaluated

            g = current_node.g + cost  # Cost to reach the neighbor
            h = heuristic[neighbor]    # Heuristic estimate from neighbor to goal
            neighbor_node = Node(neighbor, current_node, g, h)

            heapq.heappush(open_list, neighbor_node)

    return None  


graph = {
    'A': [('B', 1), ('C', 3)],
    'B': [('A', 1), ('D', 1), ('E', 3)],
    'C': [('A', 3), ('F', 2)],
    'D': [('B', 1)],
    'E': [('B', 3), ('F', 1)],
    'F': [('C', 2), ('E', 1), ('G', 1)],
    'G': [('F', 1)]
}

heuristic = {
    'A': 6,
    'B': 5,
    'C': 4,
    'D': 3,
    'E': 2,
    'F': 1,
    'G': 0  # Goal node has a heuristic of 0
}

# Run A* Algorithm
start = 'A'
goal = 'G'
path = a_star_algorithm(graph, heuristic, start, goal)

if path:
    print("Path found: ", " -> ".join(path))
else:
    print("No path found.")



#Q.4 -- Write a Program to implement best first search traversal.

import heapq

class Node:
    def __init__(self, name, h=0):
        self.name = name      
        self.h = h             

    def __lt__(self, other):
        return self.h < other.h

def best_first_search(graph, heuristic, start, goal):
    open_list = []
    visited = set()

    start_node = Node(start, heuristic[start])
    heapq.heappush(open_list, start_node)

    path = []

    while open_list:
        current_node = heapq.heappop(open_list)
        path.append(current_node.name)
        visited.add(current_node.name)

        if current_node.name == goal:
            return path

        for neighbor, cost in graph[current_node.name]:
            if neighbor not in visited:
                neighbor_node = Node(neighbor, heuristic[neighbor])
                heapq.heappush(open_list, neighbor_node)

    return None  # If no path is found

graph = {
    'A': [('B', 1), ('C', 3)],
    'B': [('A', 1), ('D', 1), ('E', 3)],
    'C': [('A', 3), ('F', 2)],
    'D': [('B', 1)],
    'E': [('B', 3), ('F', 1)],
    'F': [('C', 2), ('E', 1), ('G', 1)],
    'G': [('F', 1)]
}

heuristic = {
    'A': 6,
    'B': 5,
    'C': 4,
    'D': 3,
    'E': 2,
    'F': 1,
    'G': 0  # Goal node has a heuristic of 0
}

start = 'A'
goal = 'G'
path = best_first_search(graph, heuristic, start, goal)

if path:
    print("Best-First Search path from", start, "to", goal, ":", " -> ".join(path))
else:
    print("No path found.")


#Q.5
#Q5 -- Write a Program to implement AO* Algorithm 
class Graph:
    def __init__(self, graph, heuristic, start):
        self.graph = graph               
        self.heuristic = heuristic       
        self.start = start           
        self.solution_graph = {}         
    
    def aostar(self, node):
        print(f"Processing node: {node}")
        # Check if the node is already a solution
        if node not in self.graph or not self.graph[node]:
            return self.heuristic[node]

        min_cost = float('inf')          
        best_subtree = None              
        
        for (children, cost) in self.graph[node]:
            total_cost = cost            
            for child in children:       
                total_cost += self.aostar(child)
            
            if total_cost < min_cost:
                min_cost = total_cost
                best_subtree = children
        
        self.solution_graph[node] = best_subtree
        return min_cost

    def execute(self):
        print(f"Starting AO* algorithm from node: {self.start}\n")
        solution_cost = self.aostar(self.start)
        print(f"\nSolution Path: {self.solution_graph}")
        print(f"Solution Cost: {solution_cost}")


graph = {
    'A': [(['B', 'C'], 1), (['D'], 3)],
    'B': [(['E'], 1), (['F'], 5)],
    'C': [(['G'], 2)],
    'D': [(['H', 'I'], 2)],
    'E': [],
    'F': [],
    'G': [],
    'H': [],
    'I': []
}

heuristic = {
    'A': 10,
    'B': 6,
    'C': 4,
    'D': 7,
    'E': 1,
    'F': 2,
    'G': 12,
    'H': 3,
    'I': 4
}


start = 'A'

ao_star_graph = Graph(graph, heuristic, start)

ao_star_graph.execute()


#Assignment-3

#Q1.program to implement game playing algorithm: minimax and alpha beta pruning

import math

def minimax(depth, node_index, is_maximizing_player, values, alpha, beta):
    """
    Minimax algorithm with alpha-beta pruning.
    
    Parameters:
        depth (int): Current depth in the tree.
        node_index (int): Index of the current node.
        is_maximizing_player (bool): True if the current move is for maximizer.
        values (list): List of terminal values of leaf nodes.
        alpha (float): Best value that the maximizer can guarantee.
        beta (float): Best value that the minimizer can guarantee.
    
    Returns:
        int: The optimal value for the current player.
    """
    if depth == 3:
        return values[node_index]
    
    if is_maximizing_player:
        best = -math.inf
        
        for i in range(2):
            val = minimax(depth + 1, node_index * 2 + i, False, values, alpha, beta)
            best = max(best, val)
            alpha = max(alpha, best)
            
            if beta <= alpha:
                break
        
        return best
    else:
        best = math.inf
        
        for i in range(2):
            val = minimax(depth + 1, node_index * 2 + i, True, values, alpha, beta)
            best = min(best, val)
            beta = min(beta, best)
            
            if beta <= alpha:
                break
        
        return best


values = [3, 5, 6, 9, 1, 2, 0, -1]  # Terminal values of the leaf nodes
optimal_value = minimax(0, 0, True, values, -math.inf, math.inf)

print("The optimal value is:", optimal_value)

#Q.2
#Q2.implement nave bays model

import numpy as np

class GaussianNaiveBayes:
    def fit(self, X, y):
        """
        Fit the model to the training data.
        
        Parameters:
            X (np.ndarray): Training features, shape (n_samples, n_features).
            y (np.ndarray): Training labels, shape (n_samples,).
        """
        n_samples, n_features = X.shape
        self.classes = np.unique(y)
        n_classes = len(self.classes)
        
        self.mean = np.zeros((n_classes, n_features), dtype=np.float64)
        self.var = np.zeros((n_classes, n_features), dtype=np.float64)
        self.priors = np.zeros(n_classes, dtype=np.float64)
        
        for idx, c in enumerate(self.classes):
            X_c = X[y == c]
            self.mean[idx, :] = X_c.mean(axis=0)
            self.var[idx, :] = X_c.var(axis=0)
            self.priors[idx] = X_c.shape[0] / float(n_samples)
    
    def predict(self, X):
        """
        Predict class labels for samples in X.
        
        Parameters:
            X (np.ndarray): Test data, shape (n_samples, n_features).
        
        Returns:
            np.ndarray: Predicted class labels.
        """
        y_pred = [self._predict(x) for x in X]
        return np.array(y_pred)
    
    def _predict(self, x):
        """
        Predict the class label for a single sample.
        
        Parameters:
            x (np.ndarray): A single data point, shape (n_features,).
        
        Returns:
            int: Predicted class label.
        """
        posteriors = []
        
        for idx, c in enumerate(self.classes):
            prior = np.log(self.priors[idx])
            conditional = np.sum(np.log(self._pdf(idx, x)))
            posterior = prior + conditional
            posteriors.append(posterior)
        
        return self.classes[np.argmax(posteriors)]
    
    def _pdf(self, class_idx, x):
        """
        Probability density function for a Gaussian distribution.
        
        Parameters:
            class_idx (int): Index of the class.
            x (np.ndarray): A single data point, shape (n_features,).
        
        Returns:
            np.ndarray: Probability density for each feature.
        """
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        numerator = np.exp(- (x - mean) ** 2 / (2 * var))
        denominator = np.sqrt(2 * np.pi * var)
        return numerator / denominator

if __name__ == "__main__":
    # Training data (features and labels)
    X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8], [8, 9]])
    y_train = np.array([0, 0, 0, 1, 1, 1])
    
    X_test = np.array([[1.5, 2.5], [6.5, 7.5]])
    
    gnb = GaussianNaiveBayes()
    gnb.fit(X_train, y_train)
    y_pred = gnb.predict(X_test)
    print("Predicted class labels:", y_pred)


#Q.3.Write a Program to Implement Tower of Hanoi using Python.

def tower_of_hanoi(n, source, auxiliary, target):
   
    if n == 1:
        print(f"Move disk 1 from {source} to {target}")
        return
    

    tower_of_hanoi(n - 1, source, target, auxiliary)
    
    print(f"Move disk {n} from {source} to {target}")
    
    tower_of_hanoi(n - 1, auxiliary, source, target)


n = 3  # Number of disks
tower_of_hanoi(n, 'A', 'B', 'C')  



#Assignment-4

import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

]
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
outputs = np.array([[0], [0], [0], [1]])

np.random.seed(0)
weights = np.random.rand(2, 1)  
bias = np.random.rand(1)        

learning_rate = 0.1

epochs = 10000

for epoch in range(epochs):
    input_layer = inputs
    weighted_sum = np.dot(input_layer, weights) + bias
    predicted_output = sigmoid(weighted_sum)
    
    error = outputs - predicted_output
    
    adjustments = error * sigmoid_derivative(predicted_output)
    
    weights += np.dot(input_layer.T, adjustments) * learning_rate
    bias += np.sum(adjustments) * learning_rate

print("Trained weights: ", weights)
print("Trained bias: ", bias)

for input_data in inputs:
    weighted_sum = np.dot(input_data, weights) + bias
    output = sigmoid(weighted_sum)
    print(f"Input: {input_data} -> Output: {round(output[0])}")



#Q.2
#2. Design a Neural Network that implements a 2 input OR gate 

import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)


inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
outputs = np.array([[0], [1], [1], [1]])

np.random.seed(0)
weights = np.random.rand(2, 1)  
bias = np.random.rand(1)        

learning_rate = 0.1

epochs = 10000

for epoch in range(epochs):
    input_layer = inputs
    weighted_sum = np.dot(input_layer, weights) + bias
    predicted_output = sigmoid(weighted_sum)
    
    error = outputs - predicted_output
    
    adjustments = error * sigmoid_derivative(predicted_output)
    
    weights += np.dot(input_layer.T, adjustments) * learning_rate
    bias += np.sum(adjustments) * learning_rate

print("Trained weights: ", weights)
print("Trained bias: ", bias)

for input_data in inputs:
    weighted_sum = np.dot(input_data, weights) + bias
    output = sigmoid(weighted_sum)
    print(f"Input: {input_data} -> Output: {round(output[0])}")


#Q.3
#3. Design a Perceptron for bipolar inputs. 

import numpy as np


def bipolar_step(x):
    return 1 if x >= 0 else -1

def train_perceptron(inputs, targets, learning_rate, epochs):
   
    weights = np.random.rand(inputs.shape[1])
    bias = np.random.rand(1)
    
    for epoch in range(epochs):
        for i in range(len(inputs)):
           
            net_input = np.dot(inputs[i], weights) + bias
          
            prediction = bipolar_step(net_input)
          
            error = targets[i] - prediction
            
            weights += learning_rate * error * inputs[i]
            bias += learning_rate * error
    
    return weights, bias
inputs = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])
outputs = np.array([-1, -1, -1, 1])

learning_rate = 0.1
epochs = 10

trained_weights, trained_bias = train_perceptron(inputs, outputs, learning_rate, epochs)
print("Trained weights: ", trained_weights)
print("Trained bias: ", trained_bias)

for input_data in inputs:
    net_input = np.dot(input_data, trained_weights) + trained_bias
    output = bipolar_step(net_input)
    print(f"Input: {input_data} -> Output: {output}")


#Q.4
#4. Design a TLN for binary inputs. 

import numpy as np

def step_function(x):
    return 1 if x >= 0 else 0
def train_tln(inputs, targets, learning_rate, epochs):
   
    weights = np.random.rand(inputs.shape[1])
    bias = np.random.rand(1)
    
    for epoch in range(epochs):
        for i in range(len(inputs)):
        
            net_input = np.dot(inputs[i], weights) + bias
          
            prediction = step_function(net_input)
            error = targets[i] - prediction
            weights += learning_rate * error * inputs[i]
            bias += learning_rate * error
    
    return weights, bias

inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
outputs = np.array([0, 0, 0, 1])
learning_rate = 0.1
epochs = 10
trained_weights, trained_bias = train_tln(inputs, outputs, learning_rate, epochs)
print("Trained weights: ", trained_weights)
print("Trained bias: ", trained_bias)

for input_data in inputs:
    net_input = np.dot(input_data, trained_weights) + trained_bias
    output = step_function(net_input)
    print(f"Input: {input_data} -> Output: {output}")



#Q.5
import numpy as np
from keras.models import Sequential
from keras.layers import Dense

X = np.array([[0, 1, 1],
              [1, 0, 0],
              [1, 0, 1],
              [0, 0, 1],
              [1, 1, 1],
              [1, 0, 1],
              [0, 1, 1]])

y = np.array([1, 0, 1, 0, 1, 1, 0])  # Target output Dk

model = Sequential()
model.add(Dense(3, input_dim=3, activation='relu'))  # First hidden layer
model.add(Dense(3, activation='relu'))                # Second hidden layer
model.add(Dense(1, activation='sigmoid'))             # Output layer

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print("\nTraining for 10 epochs:")
model.fit(X, y, epochs=10, verbose=1)

print("\nTraining for 100 epochs:")
model.fit(X, y, epochs=100, verbose=1)

print("\nTraining for 10,000 epochs:")
model.fit(X, y, epochs=10000, verbose=0)  # Using verbose=0 to suppress the long output

test_input = np.array([[1, 0, 0]])  # Ensure it's a proper 2D NumPy array
predicted_output = model.predict(test_input)

print(f"\nTest Input: [1, 0, 0] -> Predicted Output: {predicted_output[0][0]}")

binary_output = np.round(predicted_output[0][0])
print(f"Rounded Predicted Output: {binary_output}")


